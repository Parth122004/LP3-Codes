# 1. Import libraries
import pandas as pd                 # data handling
import numpy as np                  # numeric ops (not strictly needed but useful)
from sklearn.preprocessing import StandardScaler  # feature scaling
from sklearn.cluster import KMeans  # k-means clustering algorithm
import matplotlib.pyplot as plt     # plotting
import seaborn as sns               # nicer plots

# 2. Load dataset
data = pd.read_csv('a5_clustering.csv', encoding='latin1')  # read CSV file
print("Dataset loaded successfully.")
print(data.head())  # show first rows to inspect columns and values

# 3. Select relevant numerical features for clustering
# choose columns that represent numeric attributes useful for clustering
features = data[['SALES', 'QUANTITYORDERED', 'PRICEEACH', 'MSRP']].dropna()  # drop rows with missing values in these cols

# 4. Normalize features for better clustering performance
scaler = StandardScaler()                 # create scaler object
scaled_features = scaler.fit_transform(features)  # fit scaler on feature data and transform it

# 5. Determine optimal number of clusters using the Elbow Method
inertia = []                              # list to store inertia (sum of squared distances)
K = range(1, 11)                          # test k from 1 to 10

for k in K:                               # loop through candidate k values
    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)  # create KMeans model
    kmeans.fit(scaled_features)           # fit model on scaled features
    inertia.append(kmeans.inertia_)      # record inertia for this k

# 6. Plot Elbow Curve
plt.figure(figsize=(8, 5))
plt.plot(K, inertia, 'bo-')               # plot k vs inertia
plt.title("Elbow Method to Find Optimal K")
plt.xlabel("Number of Clusters (k)")
plt.ylabel("Inertia (Sum of squared distances)")
plt.grid(True)
plt.show()

# 7. Train final K-Means model with chosen number of clusters
optimal_k = 3                             # choose optimal k after inspecting elbow plot
kmeans = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)  # final model
data['Cluster'] = kmeans.fit_predict(scaled_features)    # assign cluster labels to original data rows

# 8. Display sample cluster assignments
print("\nClustered Data Sample:")
print(data[['SALES', 'QUANTITYORDERED', 'PRICEEACH', 'Cluster']].head())  # preview assigned clusters

# 9. Visualize clusters on two dimensions (Sales vs Quantity)
plt.figure(figsize=(8, 6))
sns.scatterplot(
    x=data['SALES'],
    y=data['QUANTITYORDERED'],
    hue=data['Cluster'],                  # color by cluster
    palette='viridis',
    s=60
)
plt.title(f"K-Means Clusters (k={optimal_k}): Sales vs Quantity Ordered")
plt.xlabel("Sales")
plt.ylabel("Quantity Ordered")
plt.legend(title="Cluster")
plt.grid(True)
plt.show()


==============================================================================================================


# Assignment (short)

Group sales records into meaningful segments using **K-Means**.
Determine the number of clusters with the **elbow method** and visualize clusters (example plotted: Sales vs Quantity).

# Outputs (short)

* **Elbow plot:** inertia vs k. Pick k at the “elbow” (where inertia reduction slows).
* **Cluster labels:** new `Cluster` column with integer cluster id for each record.
* **Scatter plot:** points colored by cluster to inspect separation and patterns.

# Theory (simple)

* **K-Means** partitions data into k clusters by minimizing within-cluster variance (inertia).
* **Elbow method** looks for a point where increasing k yields diminishing returns in inertia.
* **Scaling** is required because K-Means uses Euclidean distance and features must be comparable.

# Viva Q&A (very short)

* Q: What is inertia?
  A: Sum of squared distances from points to their cluster centroid (lower is better).
* Q: Why scale features?
  A: To prevent large-valued features from dominating distance calculations.
* Q: How choose k?
  A: Use elbow plot, silhouette score, domain knowledge, or business constraints.
* Q: What does `n_init` do?
  A: Runs K-Means multiple times with different centroid seeds and keeps best result.
* Q: What if clusters overlap?
  A: Try different features, increase k, use other clustering methods (hierarchical, DBSCAN).

# Time & Space Complexity (simple)

* **Time:** O(n × k × i × d) — n samples, k clusters, i iterations until convergence, d features.
* **Space:** O(n × d) to store data plus O(k × d) for centroids.

# Applications (simple)

1. Customer segmentation for targeted marketing.
2. Grouping products by sales/price behavior.
3. Inventory or demand pattern discovery.
4. Market basket / basket segmentation.

---
